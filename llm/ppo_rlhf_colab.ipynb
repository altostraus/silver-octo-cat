{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\ude80 PPO Fine-Tuning with a Custom Reward Function\n",
        "This notebook shows how to fine-tune a language model using PPO from Hugging Face's `trl`, with a simple reward function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PPO Fine-Tuning of a Language Model with a Custom Reward Function\n",
        "\n",
        "# \ud83d\udee0\ufe0f Setup\n",
        "!pip install -q transformers datasets trl accelerate bitsandbytes\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "# \u2699\ufe0f Load model & tokenizer\n",
        "model_name = \"tiiuae/falcon-1b\"  # small model for quick test\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name, load_in_8bit=True, device_map=\"auto\")\n",
        "\n",
        "# \ud83d\udcc4 Load small dataset\n",
        "dataset = load_dataset(\"Abirate/english_quotes\", split=\"train[:100]\")\n",
        "\n",
        "# \ud83d\udcc8 Define a custom reward function\n",
        "def simple_reward_fn(query, response):\n",
        "    # Reward longer responses that contain \"life\"\n",
        "    reward = 1.0 if \"life\" in response.lower() else -1.0\n",
        "    reward += len(response) / 100.0  # small bonus for length\n",
        "    return reward\n",
        "\n",
        "# \u2699\ufe0f PPO Config\n",
        "config = PPOConfig(\n",
        "    model_name=model_name,\n",
        "    learning_rate=1.41e-5,\n",
        "    log_with=None,\n",
        "    mini_batch_size=1,\n",
        "    batch_size=2,\n",
        ")\n",
        "\n",
        "ppo_trainer = PPOTrainer(config, model, tokenizer)\n",
        "\n",
        "# \ud83d\udd01 Training loop\n",
        "text_prompts = [f\"What is the purpose of life?\" for _ in range(100)]\n",
        "\n",
        "for epoch, prompt in enumerate(text_prompts[:5]):  # Keep it short for test\n",
        "    query_tensors = tokenizer(prompt, return_tensors=\"pt\", padding=True).input_ids.to(model.device)\n",
        "\n",
        "    response_tensors = model.generate(query_tensors, max_new_tokens=32, pad_token_id=tokenizer.pad_token_id)\n",
        "    response_text = tokenizer.batch_decode(response_tensors[:, query_tensors.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "    rewards = [simple_reward_fn(prompt, response_text[0])]\n",
        "    print(f\"Epoch {epoch+1}: {response_text[0]} | Reward: {rewards[0]}\")\n",
        "\n",
        "    ppo_trainer.step([prompt], response_text, rewards)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}